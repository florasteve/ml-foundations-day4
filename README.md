# ML Foundations — Day 4: Gradient Descent
Implement gradient descent from scratch in NumPy and visualize loss surfaces (2D/3D), learning-rate effects, convergence, and optimization intuition.

## Status
Project complete – Documentation in progress

## Roadmap

## Results
Here are key visualizations generated by the Day 4 demos:

- Vanilla GD on convex/quadratic
- Logistic regression GD
- 2D/3D loss surface plots
- Learning-rate sweep + divergence
- Batch vs stochastic vs mini-batch (preview)

## Setup
python -m venv .venv && source .venv/bin/activate
pip install numpy matplotlib jupyter
### Quadratic Loss (Contour + GD Path)
![Quadratic GD Path](figures/quadratic_gd_path.png)

### Loss vs Iteration
![Quadratic GD Loss](figures/quadratic_gd_loss.png)

### Learning Rate Sweep
Example of different learning rates:
![LR Sweep 0.05–1.5](figures/quadratic_lr_0p05.png)

### 3D Surface of Quadratic Bowl
![Quadratic 3D Surface](figures/quadratic_surface3d.png)

### Logistic Regression Loss Surface
![Logistic Loss Surface](figures/logreg_surface.png)

### GD Path on Logistic Loss
![Logistic GD Path](figures/logreg_gd_path.png)
